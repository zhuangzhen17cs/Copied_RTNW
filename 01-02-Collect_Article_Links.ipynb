{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Collect Articles\n",
    "Next, we collect recent articles by scraping different sites such as EDN Network, EE Times, Electronic Design, Electronics Weekly. Site links can be found in ./data/Artical_Links.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import hashlib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_scraper():\n",
    "    state = {}\n",
    "\n",
    "    state[\"headers\"] = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    state[\"cutoff_date\"] = datetime(2025, 4, 1)\n",
    "    state[\"articles\"] = []\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument(f'--user-agent={state[\"headers\"][\"User-Agent\"]}')\n",
    "    state[\"chrome_options\"] = chrome_options\n",
    "\n",
    "    os.makedirs('./intermediate_data', exist_ok=True)\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selenium_driver(state):\n",
    "#\"\"\"Initialize Selenium Chrome driver\"\"\"\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=state[\"chrome_options\"])\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize Chrome driver: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request for pages from sites using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_source_selenium(state, url, wait_time=10):\n",
    "#Get page source using Selenium\n",
    "    driver = get_selenium_driver(state)\n",
    "    if not driver:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, wait_time).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "        return page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Selenium failed for {url}: {e}\")\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request for pages from sites using retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_requests(state, url, retries=3):\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            session.headers.update(state[\"headers\"])\n",
    "            response = session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Requests failed for {url}: {e}\")\n",
    "                return None\n",
    "            time.sleep(2)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_page_content(state, url):\n",
    "    content = get_page_source_selenium(state,url)\n",
    "    if not content:\n",
    "        content = get_page_requests(state,url)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced date parsing (Non-RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "\n",
    "    if not date_str:\n",
    "        return None\n",
    "\n",
    "    # Clean the date string\n",
    "    date_str = re.sub(r'Posted on|Modified on|Published|By.*', '', date_str, flags=re.IGNORECASE)\n",
    "    date_str = date_str.strip()\n",
    "\n",
    "    # Common date patterns\n",
    "    patterns = [\n",
    "        r'(\\d{1,2})(?:st|nd|rd|th)?\\s+(\\w+)\\s+(\\d{4})',  # 3rd June 2025\n",
    "        r'(\\w+)\\s+(\\d{1,2}),?\\s+(\\d{4})',  # June 3, 2025\n",
    "        r'(\\d{4})-(\\d{2})-(\\d{2})',  # 2025-06-03\n",
    "        r'(\\d{2})/(\\d{2})/(\\d{4})',  # 06/03/2025\n",
    "        r'(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',  # 3 June 2025\n",
    "        r'(\\w{3})\\s+(\\d{1,2}),?\\s+(\\d{4})',  # Jun 3, 2025\n",
    "    ]\n",
    "\n",
    "    months = {\n",
    "        'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6,\n",
    "        'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12,\n",
    "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "        'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "    }\n",
    "\n",
    "    date_str_lower = date_str.lower()\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, date_str_lower)\n",
    "        if match:\n",
    "            try:\n",
    "                groups = match.groups()\n",
    "                if len(groups) == 3:\n",
    "                    if groups[0].isdigit() and groups[2].isdigit():  # Day Month Year\n",
    "                        day, month, year = int(groups[0]), groups[1], int(groups[2])\n",
    "                        if month in months:\n",
    "                            return datetime(year, months[month], day)\n",
    "                    elif groups[1].isdigit():  # Month Day Year\n",
    "                        month, day, year = groups[0], int(groups[1]), int(groups[2])\n",
    "                        if month in months:\n",
    "                            return datetime(year, months[month], day)\n",
    "                    elif '-' in date_str or '/' in date_str:\n",
    "                        if len(groups[0]) == 4:  # YYYY-MM-DD\n",
    "                            return datetime(int(groups[0]), int(groups[1]), int(groups[2]))\n",
    "                        else:  # MM/DD/YYYY\n",
    "                            return datetime(int(groups[2]), int(groups[0]), int(groups[1]))\n",
    "            except:\n",
    "                continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape EE Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_eetimes(state):\n",
    "\n",
    "    articles = []\n",
    "    # RSS feed\n",
    "    rss_urls = [\n",
    "        \"https://www.eetimes.com/feed/\",\n",
    "        \"https://www.eetimes.com/rss/\"\n",
    "    ]\n",
    "\n",
    "    for rss_url in rss_urls:\n",
    "        try:\n",
    "            response = requests.get(rss_url, headers=state[\"headers\"], timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                items = soup.find_all('item')\n",
    "                \n",
    "                for item in items:\n",
    "                    try:\n",
    "                        title = item.find('title').text.strip()\n",
    "                        link = item.find('link').text.strip()\n",
    "                        pub_date = item.find('pubDate')\n",
    "                        \n",
    "                        if pub_date:\n",
    "                            date_str = pub_date.text\n",
    "                            # Parse RSS date format: Wed, 03 Jun 2025 10:00:00 GMT\n",
    "                            article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                            \n",
    "                            if article_date > state[\"cutoff_date\"]:\n",
    "                                articles.append({\n",
    "                                    'title': title,\n",
    "                                    'url': link,\n",
    "                                    'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                    'source': 'EE Times'\n",
    "                                })\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# Direct scraping\n",
    "        if not articles:\n",
    "            content = get_page_content(state,\"https://www.eetimes.com\")\n",
    "            if content:\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                \n",
    "                # Look for article links\n",
    "                selectors = [\n",
    "                    'article .entry-title a', 'h2.entry-title a', \n",
    "                    '.post-title a', 'h3 a', '.river-block h3 a'\n",
    "                ]\n",
    "                for selector in selectors:\n",
    "                    links = soup.select(selector)\n",
    "                    for link in links:\n",
    "                        href = link.get('href')\n",
    "                        title = link.get_text(strip=True)\n",
    "                        \n",
    "                        if href and title and 'eetimes.com' in href:\n",
    "                            articles.append({\n",
    "                                'title': title,\n",
    "                                'url': href,\n",
    "                                'date': datetime.now().strftime('%Y-%m-%d'),  # Fallback date\n",
    "                                'source': 'EE Times'\n",
    "                            })\n",
    "                    if links:  # If we found articles with this selector, break\n",
    "                        break\n",
    "    return articles[:20]  # Limit to 20 most recent articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Electronics Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_electronics_weekly(state):\n",
    "\n",
    "    articles = []\n",
    "    #RSS first\n",
    "    try:\n",
    "        response = requests.get(\"https://www.electronicsweekly.com/feed/\", \n",
    "                                headers=state[\"headers\"], timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'xml')\n",
    "            items = soup.find_all('item')\n",
    "            \n",
    "            for item in items:\n",
    "                try:\n",
    "                    title = item.find('title').text.strip()\n",
    "                    link = item.find('link').text.strip()\n",
    "                    pub_date = item.find('pubDate')\n",
    "                    \n",
    "                    if pub_date:\n",
    "                        date_str = pub_date.text\n",
    "                        article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                        \n",
    "                        if article_date > state[\"cutoff_date\"]:\n",
    "                            articles.append({\n",
    "                                'title': title,\n",
    "                                'url': link,\n",
    "                                'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                'source': 'Electronics Weekly'\n",
    "                            })\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except Exception:\n",
    "        # Fallback to direct scraping\n",
    "        content = get_page_content(state, \"https://www.electronicsweekly.com\")\n",
    "        if content:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            article_links = soup.select('article h2 a, .entry-title a')\n",
    "            for link in article_links:\n",
    "                href = link.get('href')\n",
    "                title = link.get_text(strip=True)\n",
    "                \n",
    "                if href and title:\n",
    "                    full_url = urljoin(\"https://www.electronicsweekly.com\", href)\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': full_url,\n",
    "                        'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                        'source': 'Electronics Weekly'\n",
    "                    })\n",
    "\n",
    "    return articles[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape EDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_edn_network(state):\n",
    "    \n",
    "    articles = []\n",
    "    #RSS feed\n",
    "    try:\n",
    "        response = requests.get(\"https://www.edn.com/feed/\", \n",
    "                                headers=state[\"headers\"], timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'xml')\n",
    "            items = soup.find_all('item')\n",
    "            \n",
    "            for item in items:\n",
    "                try:\n",
    "                    title = item.find('title').text.strip()\n",
    "                    link = item.find('link').text.strip()\n",
    "                    pub_date = item.find('pubDate')\n",
    "                    \n",
    "                    if pub_date:\n",
    "                        date_str = pub_date.text\n",
    "                        article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                        \n",
    "                        if article_date > state[\"cutoff_date\"]:\n",
    "                            articles.append({\n",
    "                                'title': title,\n",
    "                                'url': link,\n",
    "                                'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                'source': 'EDN Network'\n",
    "                            })\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except Exception:\n",
    "        # Fallback to direct scraping\n",
    "        content = get_page_content(state, \"https://www.edn.com\")\n",
    "        if content:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            article_links = soup.select('.river-block h3 a, article h2 a')\n",
    "            for link in article_links:\n",
    "                href = link.get('href')\n",
    "                title = link.get_text(strip=True)\n",
    "                \n",
    "                if href and title:\n",
    "                    full_url = urljoin(\"https://www.edn.com\", href)\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': full_url,\n",
    "                        'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                        'source': 'EDN Network'\n",
    "                    })\n",
    "\n",
    "    return articles[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Electronic Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_electronic_design(state):\n",
    "\n",
    "    articles = []\n",
    "    # RSS feed\n",
    "    try:\n",
    "        response = requests.get(\"https://www.electronicdesign.com/rss.xml\", \n",
    "                                headers=state[\"headers\"], timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'xml')\n",
    "            items = soup.find_all('item')\n",
    "            \n",
    "            for item in items:\n",
    "                try:\n",
    "                    title = item.find('title').text.strip()\n",
    "                    link = item.find('link').text.strip()\n",
    "                    pub_date = item.find('pubDate')\n",
    "                    \n",
    "                    if pub_date:\n",
    "                        date_str = pub_date.text\n",
    "                        article_date = datetime.strptime(date_str.split(',')[1].strip()[:11], '%d %b %Y')\n",
    "                        \n",
    "                        if article_date > state[\"cutoff_date\"]:\n",
    "                            articles.append({\n",
    "                                'title': title,\n",
    "                                'url': link,\n",
    "                                'date': article_date.strftime('%Y-%m-%d'),\n",
    "                                'source': 'Electronic Design'\n",
    "                            })\n",
    "                except Exception:\n",
    "                    continue\n",
    "    except Exception:\n",
    "        # Fallback to direct scraping\n",
    "        content = get_page_content(state, \"https://www.electronicdesign.com\")\n",
    "        if content:\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            article_links = soup.select('article h2 a, .post-title a')\n",
    "            for link in article_links:\n",
    "                href = link.get('href')\n",
    "                title = link.get_text(strip=True)\n",
    "                \n",
    "                if href and title:\n",
    "                    full_url = urljoin(\"https://www.electronicdesign.com\", href)\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': full_url,\n",
    "                        'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                        'source': 'Electronic Design'\n",
    "                    })\n",
    "\n",
    "    return articles[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to scrape all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_sites():\n",
    "    \n",
    "    state = init_scraper()\n",
    "    all_articles = []\n",
    "\n",
    "    sites = [\n",
    "        (\"EE Times\", scrape_eetimes),\n",
    "        (\"Electronics Weekly\",scrape_electronics_weekly),\n",
    "        (\"EDN Network\", scrape_edn_network),\n",
    "        (\"Electronic Design\", scrape_electronic_design)\n",
    "    ]\n",
    "\n",
    "    for site_name, scrape_func in sites:\n",
    "        try:\n",
    "            articles = scrape_func(state)\n",
    "            all_articles.extend(articles)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {site_name}: {e}\")\n",
    "        time.sleep(2)  # Rate limiting between sites\n",
    "\n",
    "    # Remove duplicates\n",
    "    seen_urls = set()\n",
    "    unique_articles = []\n",
    "    for article in all_articles:\n",
    "        if article['url'] not in seen_urls:\n",
    "            unique_articles.append(article)\n",
    "            seen_urls.add(article['url'])\n",
    "    # Sort by date (newest first)\n",
    "    unique_articles.sort(key=lambda x: x['date'], reverse=True)\n",
    "\n",
    "# Return the list\n",
    "    return unique_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Relevant Article Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = scrape_all_sites()\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv('./intermediate_data/Scraped_Article_Links.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape articles based on keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_with_keywords(state, keywords):\n",
    "\n",
    "    all_articles = []\n",
    "\n",
    "    scraping_functions = [\n",
    "        scrape_eetimes,\n",
    "        scrape_electronics_weekly,\n",
    "        scrape_edn_network,\n",
    "        scrape_electronic_design\n",
    "    ]\n",
    "\n",
    "    for scraper in scraping_functions:\n",
    "        try:\n",
    "            articles = scraper(state)\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    article_text = f\"{article['title']}\"\n",
    "                    # Optionally fetch content for deeper match\n",
    "                    content = get_page_content(state, article['url'])\n",
    "                    if content:\n",
    "                        soup = BeautifulSoup(content, 'html.parser')\n",
    "                        body_text = soup.get_text(separator=' ', strip=True)\n",
    "                        article_text += \" \" + body_text.lower()\n",
    "\n",
    "                    if any(keyword in article_text.lower() for keyword in keywords):\n",
    "                        all_articles.append(article)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape site: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trade relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_scraper()\n",
    "tarrif_keywords = [\"tariff\", \"trade war\", \"export\", \"import duty\"]\n",
    "tarrif_articles = scrape_articles_with_keywords(state, tarrif_keywords)\n",
    "\n",
    "df = pd.DataFrame(tarrif_articles)\n",
    "df.to_csv('./intermediate_data/Tarrif_Articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supply relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "supply_keywords = [\"shortage\", \"supply chain\", \"distribution\", \"logistics\"]\n",
    "supply_articles = scrape_articles_with_keywords(state, supply_keywords)\n",
    "\n",
    "df = pd.DataFrame(supply_articles)\n",
    "df.to_csv('./intermediate_data/Supply_Articles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
