{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Retreive Article Data\n",
    "\n",
    "This notebook scrapes article links and retreives text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "import random\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import urllib3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "import cloudscraper\n",
    "import httpx\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppress SSL warning and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List extreme domains and proxy list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problematic domains that need extreme methods\n",
    "EXTREME_DOMAINS = ['www.eetimes.com', 'www.edn.com']\n",
    "\n",
    "# Free proxy list\n",
    "PROXY_LIST = [\n",
    "    # Add working proxies here if available\n",
    "    # 'http://proxy1:port',\n",
    "    # 'http://proxy2:port',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloudscraper_session():\n",
    "    try:\n",
    "        scraper = cloudscraper.create_scraper(\n",
    "            browser={\n",
    "                'browser': 'chrome',\n",
    "                'platform': 'windows',\n",
    "                'desktop': True\n",
    "            }\n",
    "        )\n",
    "        return scraper\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create cloudscraper: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_selenium_driver():\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')  # Run in background\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create Selenium driver: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_with_retries():\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "            backoff_factor=1\n",
    "        )\n",
    "    except TypeError:\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            backoff_factor=1\n",
    "        )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract article text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(html_content, url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Remove unwanted elements\n",
    "    for script in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\", \"form\", \"iframe\", \"noscript\"]):\n",
    "        script.decompose()\n",
    "    selectors = [\n",
    "        'article',\n",
    "        '[role=\"main\"]',\n",
    "        '.article-content',\n",
    "        '.post-content',\n",
    "        '.entry-content',\n",
    "        '.content',\n",
    "        '.main-content',\n",
    "        '#content',\n",
    "        '.article-body',\n",
    "        '.story-body',\n",
    "        '.post-body',\n",
    "        '.article-text',\n",
    "        '.body-content',\n",
    "        '.article-wrapper'\n",
    "    ]\n",
    "    text_content = \"\"\n",
    "\n",
    "    # Process each selector\n",
    "    for selector in selectors:\n",
    "        elements = soup.select(selector)\n",
    "        if elements:\n",
    "            for element in elements:\n",
    "                text_content += element.get_text(separator=' ', strip=True) + \" \"\n",
    "            break\n",
    "    # Fallback to paragraphs\n",
    "    if not text_content.strip():\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30])\n",
    "    # Last resort - get body text\n",
    "    if not text_content.strip():\n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            text_content = body.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    return text_content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_with_requests(url, session):\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'\n",
    "    ]\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(user_agents),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Pragma': 'no-cache'\n",
    "    }\n",
    "    response = session.get(url, headers=headers, timeout=(15, 45), verify=False, allow_redirects=True)\n",
    "    response.raise_for_status()\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_cloudscraper(url, scraper):\n",
    "    if scraper is None:\n",
    "        raise Exception(\"CloudScraper not available\")\n",
    "    response = scraper.get(url, timeout=45)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "async def scrape_with_httpx(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=45.0, verify=False) as client:\n",
    "        response = await client.get(url, headers=headers, follow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with selenium driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_selenium(url, driver):\n",
    "    if driver is None:\n",
    "        raise Exception(\"Selenium driver not available\")\n",
    "    driver.set_page_load_timeout(60)\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"p\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            pass  # Continue anyway\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
    "    time.sleep(2)\n",
    "    html_content = driver.page_source\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape with httpx sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_httpx_sync(url):\n",
    "    \"\"\"Synchronous version of HTTPX scraping\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    with httpx.Client(timeout=45.0, verify=False) as client:\n",
    "        response = client.get(url, headers=headers, follow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme method to scrape an article (Uses 4 different methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_extreme(url, session=None, scraper=None, driver=None):\n",
    "    methods = []\n",
    "    if session:\n",
    "        methods.append((\"Requests\", lambda: scrape_with_requests(url, session)))\n",
    "    \n",
    "    if scraper:\n",
    "        methods.append((\"CloudScraper\", lambda: scrape_with_cloudscraper(url, scraper)))\n",
    "    methods.append((\"HTTPX\", lambda: scrape_with_httpx_sync(url)))\n",
    "    \n",
    "    if driver:\n",
    "        methods.append((\"Selenium\", lambda: scrape_with_selenium(url, driver)))\n",
    "    \n",
    "    for method_name, method_func in methods:\n",
    "        try:\n",
    "            html_content = method_func()\n",
    "            if html_content and len(html_content) > 1000:\n",
    "                article_text = extract_article_text(html_content, url)\n",
    "                if len(article_text) > 100:\n",
    "                    return article_text\n",
    "                else:\n",
    "                    logging.warning(f\"{method_name} got content but extraction failed\")\n",
    "            else:\n",
    "                logging.warning(f\"{method_name} returned insufficient content\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def is_extreme_domain(url):\n",
    "    return any(domain in url.lower() for domain in EXTREME_DOMAINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape all articles with extreme methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url, session=None, scraper=None, driver=None):\n",
    "    # For extreme domains, use all methods\n",
    "    if is_extreme_domain(url):\n",
    "        result = scrape_article_extreme(url, session, scraper, driver)\n",
    "        if result:\n",
    "            return result\n",
    "        else:\n",
    "            logging.error(f\"All extreme methods failed for {url}\")\n",
    "            return \"Content unavailable - all methods failed\"\n",
    "    # For normal domains, use regular method\n",
    "    try:\n",
    "        if session is None:\n",
    "            session = create_session_with_retries()\n",
    "        \n",
    "        html_content = scrape_with_requests(url, session)\n",
    "        article_text = extract_article_text(html_content, url)\n",
    "        \n",
    "        if len(article_text) > 100:\n",
    "            return article_text\n",
    "        else:\n",
    "            result = scrape_article_extreme(url, session, scraper, driver)\n",
    "            return result if result else \"Content unavailable - extraction failed\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        result = scrape_article_extreme(url, session, scraper, driver)\n",
    "        return result if result else f\"Content unavailable - {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect and organize article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_article_data():\n",
    "    # Read the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv('./intermediate_data/Scraped_Article_Links.csv')\n",
    "        logging.info(f\"Loaded {len(df)} articles from CSV\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['title', 'url', 'date', 'source']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"Missing required columns: {missing_columns}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize all scraping tools\n",
    "    session = create_session_with_retries()\n",
    "    scraper = create_cloudscraper_session()\n",
    "    driver = create_selenium_driver()\n",
    "     \n",
    "    articles_data = []\n",
    "    successful_scrapes = 0\n",
    "    failed_scrapes = 0\n",
    "    try:\n",
    "        # Process each article\n",
    "        for index, row in df.iterrows():\n",
    "            logging.info(f\"Processing article {index + 1}/{len(df)}: {row['title'][:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                article_text = scrape_article(row['url'], session, scraper, driver)\n",
    "                \n",
    "                article_data = {\n",
    "                    'title': row['title'],\n",
    "                    'source': row['source'],\n",
    "                    'date': row['date'],\n",
    "                    'link': row['url'],\n",
    "                    'text': article_text,\n",
    "                }\n",
    "                \n",
    "                articles_data.append(article_data)\n",
    "                \n",
    "                if len(article_text) > 100 and not article_text.startswith(\"Content unavailable\"):\n",
    "                    successful_scrapes += 1\n",
    "                else:\n",
    "                    failed_scrapes += 1\n",
    "                    \n",
    "                # Progress update\n",
    "                if (index + 1) % 5 == 0:\n",
    "                    logging.info(f\"Progress: {index + 1}/{len(df)} - Success: {successful_scrapes}, Failed: {failed_scrapes}\")\n",
    "                \n",
    "                # Respectful delay\n",
    "                if is_extreme_domain(row['url']):\n",
    "                    time.sleep(random.uniform(2, 4))  # Longer for extreme domains\n",
    "                else:\n",
    "                    time.sleep(random.uniform(0.5, 1.5))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                article_data = {\n",
    "                    'title': row['title'],\n",
    "                    'source': row['source'],\n",
    "                    'date': row['date'],\n",
    "                    'link': row['url'],\n",
    "                    'text': f\"Processing error: {str(e)}\",\n",
    "                }\n",
    "                articles_data.append(article_data)\n",
    "                failed_scrapes += 1\n",
    "    finally:\n",
    "        # Clean up Selenium driver\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Final summary\n",
    "    logging.info(f\"Total articles processed: {len(articles_data)}\")\n",
    "    \n",
    "    return articles_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and Save raw article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 20:33:57,456 - INFO - Loaded 30 articles from CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 20:34:00,918 - INFO - Processing article 1/30: Assessing CHIPS Acts Value, Tariffs and Semi Equip...\n",
      "2025-06-07 20:34:46,352 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /assessing-chips-acts-value-tariffs-and-semi-equipment-growth/\n",
      "2025-06-07 20:35:33,583 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /assessing-chips-acts-value-tariffs-and-semi-equipment-growth/\n",
      "2025-06-07 20:36:23,034 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /assessing-chips-acts-value-tariffs-and-semi-equipment-growth/\n",
      "2025-06-07 20:38:59,592 - INFO - Processing article 2/30: China Weaponizes Rare Earths, Hitting Factories...\n",
      "2025-06-07 20:39:00,742 - WARNING - Requests got content but extraction failed\n",
      "2025-06-07 20:40:42,781 - INFO - Processing article 3/30: Gadget Book: Practical Deep Learning (with Python)...\n",
      "2025-06-07 20:40:44,488 - INFO - Processing article 4/30: Vehicle hardware security certified to ISO/SAE 214...\n",
      "2025-06-07 20:40:45,768 - INFO - Processing article 5/30: DigiKey offers Zephyr RTOS workshop, video trainin...\n",
      "2025-06-07 20:40:46,105 - INFO - Progress: 5/30 - Success: 5, Failed: 0\n",
      "2025-06-07 20:40:47,562 - INFO - Processing article 6/30: Genesis of European semi manufacturing  clean-up...\n",
      "2025-06-07 20:40:48,679 - INFO - Processing article 7/30: Most Read – STEP for Semiconductor, Tomahawk 6, St...\n",
      "2025-06-07 20:40:51,558 - INFO - Processing article 8/30: Rare earths export restrictions hit car production...\n",
      "2025-06-07 20:40:53,032 - INFO - Processing article 9/30: Imec researchers demo single-chip optical and micr...\n",
      "2025-06-07 20:40:54,983 - INFO - Processing article 10/30: Robots to enable urban mining of CRMs...\n",
      "2025-06-07 20:40:56,420 - INFO - Progress: 10/30 - Success: 10, Failed: 0\n",
      "2025-06-07 20:40:57,306 - INFO - Processing article 11/30: European SPACE4Cities names 20 winning urban plann...\n",
      "2025-06-07 20:40:58,171 - INFO - Processing article 12/30: Intel’s Tech Predictions In 2009...\n",
      "2025-06-07 20:40:59,224 - INFO - Processing article 13/30: AI and the problem of misinformation...\n",
      "2025-06-07 20:40:59,856 - WARNING - Requests got content but extraction failed\n",
      "2025-06-07 20:42:46,435 - INFO - Processing article 14/30: Standardization and modularization in pick and pla...\n",
      "2025-06-07 20:43:31,446 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /standardization-and-modularization-in-pick-and-place-equipment/\n",
      "2025-06-07 20:43:51,658 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /standardization-and-modularization-in-pick-and-place-equipment/\n",
      "2025-06-07 20:44:40,874 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /standardization-and-modularization-in-pick-and-place-equipment/\n",
      "2025-06-07 20:46:41,267 - INFO - Processing article 15/30: GlobalFoundries Pledges $16 Billion U.S. Investmen...\n",
      "2025-06-07 20:47:26,280 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /globalfoundries-pledges-16-billion-u-s-investment-with-trumps-help/\n",
      "2025-06-07 20:48:13,862 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /globalfoundries-pledges-16-billion-u-s-investment-with-trumps-help/\n",
      "2025-06-07 20:49:03,057 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /globalfoundries-pledges-16-billion-u-s-investment-with-trumps-help/\n",
      "2025-06-07 20:51:29,521 - INFO - Progress: 15/30 - Success: 15, Failed: 0\n",
      "2025-06-07 20:51:32,513 - INFO - Processing article 16/30: Jim Keller: ‘Whatever Nvidia Does, We’ll Do The Op...\n",
      "2025-06-07 20:52:17,715 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /jim-keller-whatever-nvidia-does-well-do-the-opposite/\n",
      "2025-06-07 20:53:04,850 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /jim-keller-whatever-nvidia-does-well-do-the-opposite/\n",
      "2025-06-07 20:53:53,993 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /jim-keller-whatever-nvidia-does-well-do-the-opposite/\n",
      "2025-06-07 20:56:20,088 - INFO - Processing article 17/30: OpenGMSL Aims to Overcome Automotive Video Interop...\n",
      "2025-06-07 20:56:21,168 - WARNING - Requests got content but extraction failed\n",
      "2025-06-07 20:57:35,450 - INFO - Processing article 18/30: The Opportunity for Liquid Sensing...\n",
      "2025-06-07 20:58:20,462 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /the-opportunity-for-liquid-sensing/\n",
      "2025-06-07 20:59:07,787 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /the-opportunity-for-liquid-sensing/\n",
      "2025-06-07 20:59:57,027 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /the-opportunity-for-liquid-sensing/\n",
      "2025-06-07 21:01:57,459 - INFO - Processing article 19/30: DMMs improve accuracy and usability...\n",
      "2025-06-07 21:01:59,389 - WARNING - Requests got content but extraction failed\n",
      "2025-06-07 21:03:40,679 - INFO - Processing article 20/30: GaN HEMT earns DLA-JANS certification...\n",
      "2025-06-07 21:04:25,687 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /gan-hemt-earns-dla-jans-certification/\n",
      "2025-06-07 21:05:12,835 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /gan-hemt-earns-dla-jans-certification/\n",
      "2025-06-07 21:06:02,085 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /gan-hemt-earns-dla-jans-certification/\n",
      "2025-06-07 21:07:57,536 - INFO - Progress: 20/30 - Success: 20, Failed: 0\n",
      "2025-06-07 21:08:00,814 - INFO - Processing article 21/30: Multichannel impedance meter analyzes Li-Ion cells...\n",
      "2025-06-07 21:08:01,042 - WARNING - Requests got content but extraction failed\n",
      "2025-06-07 21:09:12,347 - INFO - Processing article 22/30: Motor gate drivers enable flexible current control...\n",
      "2025-06-07 21:09:57,354 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /motor-gate-drivers-enable-flexible-current-control/\n",
      "2025-06-07 21:10:44,500 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /motor-gate-drivers-enable-flexible-current-control/\n",
      "2025-06-07 21:11:33,662 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /motor-gate-drivers-enable-flexible-current-control/\n",
      "2025-06-07 21:13:58,669 - INFO - Processing article 23/30: Powerline module enables EV charger data links...\n",
      "2025-06-07 21:13:58,885 - WARNING - Requests got content but extraction failed\n",
      "2025-06-07 21:15:39,652 - INFO - Processing article 24/30: A quick and practical view of USB Power Delivery (...\n",
      "2025-06-07 21:16:24,663 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /a-quick-and-practical-view-of-usb-power-delivery-usb-pd-design/\n",
      "2025-06-07 21:17:11,928 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /a-quick-and-practical-view-of-usb-power-delivery-usb-pd-design/\n",
      "2025-06-07 21:18:01,303 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /a-quick-and-practical-view-of-usb-power-delivery-usb-pd-design/\n",
      "2025-06-07 21:20:27,478 - INFO - Processing article 25/30: Ambature to Deliver Superconductor IP for AI’s Ene...\n",
      "2025-06-07 21:21:12,662 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /ambature-to-deliver-superconductor-ip-for-ais-energy-problem/\n",
      "2025-06-07 21:21:59,849 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /ambature-to-deliver-superconductor-ip-for-ais-energy-problem/\n",
      "2025-06-07 21:22:48,994 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /ambature-to-deliver-superconductor-ip-for-ais-energy-problem/\n",
      "2025-06-07 21:25:19,465 - INFO - Progress: 25/30 - Success: 25, Failed: 0\n",
      "2025-06-07 21:25:22,681 - INFO - Processing article 26/30: Arm Launches Zena to Accelerate AI-Defined Vehicle...\n",
      "2025-06-07 21:26:07,957 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /arm-launches-zena-to-accelerate-ai-defined-vehicle/\n",
      "2025-06-07 21:26:55,161 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /arm-launches-zena-to-accelerate-ai-defined-vehicle/\n",
      "2025-06-07 21:27:44,544 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /arm-launches-zena-to-accelerate-ai-defined-vehicle/\n",
      "2025-06-07 21:30:16,758 - INFO - Processing article 27/30: 10-octave linear-in-pitch VCO with buffered tri-wa...\n",
      "2025-06-07 21:31:02,512 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /10-octave-linear-in-pitch-vco-with-buffered-tri-wave-output/\n",
      "2025-06-07 21:31:50,279 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /10-octave-linear-in-pitch-vco-with-buffered-tri-wave-output/\n",
      "2025-06-07 21:32:39,488 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /10-octave-linear-in-pitch-vco-with-buffered-tri-wave-output/\n",
      "2025-06-07 21:35:10,843 - INFO - Processing article 28/30: Seeing inside entry-level audiophile desire: Monop...\n",
      "2025-06-07 21:35:56,288 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /seeing-inside-entry-level-audiophile-desire-monoprices-liquid-spark-headphone-amplifier/\n",
      "2025-06-07 21:36:43,541 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /seeing-inside-entry-level-audiophile-desire-monoprices-liquid-spark-headphone-amplifier/\n",
      "2025-06-07 21:37:32,721 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.edn.com', port=443): Read timed out. (read timeout=45)\")': /seeing-inside-entry-level-audiophile-desire-monoprices-liquid-spark-headphone-amplifier/\n",
      "2025-06-07 21:40:11,926 - INFO - Processing article 29/30: Smart Design Tips for IoT Nodes to Last for Decade...\n",
      "2025-06-07 21:40:16,603 - INFO - Processing article 30/30: Indian Watchmaker Bets Big on Customized Manufactu...\n",
      "2025-06-07 21:41:01,622 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /indian-watchmaker-bets-big-on-customized-manufacturing-lines/\n",
      "2025-06-07 21:41:48,793 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /indian-watchmaker-bets-big-on-customized-manufacturing-lines/\n",
      "2025-06-07 21:42:37,966 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.eetimes.com', port=443): Read timed out. (read timeout=45)\")': /indian-watchmaker-bets-big-on-customized-manufacturing-lines/\n",
      "2025-06-07 21:45:10,425 - INFO - Progress: 30/30 - Success: 30, Failed: 0\n",
      "2025-06-07 21:45:16,199 - INFO - Total articles processed: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully\n"
     ]
    }
   ],
   "source": [
    "collected_data = collect_article_data()\n",
    "with open('./intermediate_data/Scraped_Article_Raw_Data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(collected_data, f, indent=2, ensure_ascii=False)\n",
    "print(\"Data saved successfully\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (recommendation_libraries)",
   "language": "python",
   "name": "recommendation_libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
